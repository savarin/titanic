# Titanic - Getting 0.799 with Random Forests and Gradient Boosting

## Details

The materials here build on Section 1-5 the [Kaggle Berlin Introductory Tutorial](https://github.com/savarin/kaggleberlin-introtutorial).
comprising parameter-tuned implementations of Random Forests and Gradient Boosting, as
well as the ensemble of both models.

## Tutorial

The tutorial provides a more detailed, step-by-step explanation:  
- [Section 1-0 - First Cut.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-0%20-%20First%20Cut.ipynb)
- [Section 1-1 - Filling-in Missing Values.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-1%20-%20Filling-in%20Missing%20Values.ipynb)
- [Section 1-2 - Creating Dummy Variables.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-2%20-%20Creating%20Dummy%20Variables.ipynb)
- [Section 1-3 - Parameter Tuning.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-3%20-%20Parameter%20Tuning.ipynb)
- [Section 1-4 - Building Pipelines.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-4%20-%20Building%20Pipelines.ipynb)
- [Section 1-5 - Final Checks.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Section%201-5%20-%20Final%20Checks.ipynb)

In addition, further discussion is provided on cross-validation and visualisation:
- [Appendix A - Cross-Validation.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Appendix%20A%20-%20Cross-Validation.ipynb)
- [Appendix B - Visualisation.ipynb](http://nbviewer.ipython.org/github/savarin/kaggleberlin-introtutorial/blob/master/notebooks/Appendix%20B%20-%20Visualisation.ipynb)